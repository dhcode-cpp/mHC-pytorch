{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea28d27-33b4-40de-a26c-65fc8b0d9d49",
   "metadata": {},
   "source": [
    "# mHC-Pytorch \n",
    "\n",
    "mHC: [mHC: Manifold-Constrained Hyper-Connections](https://arxiv.org/abs/2512.24880)\n",
    "\n",
    "git: [dhcode-cpp/mHC-pytorch](https://github.com/dhcode-cpp/mHC-pytorch)\n",
    "\n",
    "blog: [【手撕 mHC】详解DeepSeek残差链接mHC进化之路（超长文、附代码）](https://zhuanlan.zhihu.com/p/1990683672337223894)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a69f23-3d8b-486f-abb1-be92780e1854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1179c0e70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e3de10-1b64-4975-9a28-86cceba92712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "\n",
    "dim = 512\n",
    "rate = 2\n",
    "layer_id = 10\n",
    "dynamic = True\n",
    "\n",
    "bsz = 1\n",
    "seq_len = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151fb64-a114-4161-a620-da59a2b0b085",
   "metadata": {},
   "source": [
    "## Manifold-Constrained Hyper-Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a13531-3224-418d-bc5c-33a7c1bb407d",
   "metadata": {},
   "source": [
    "## Math\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\vec{\\mathbf{x}}'_l = \\text{RMSNorm}(\\vec{\\mathbf{x}}_l) \\\\\n",
    "        \\tilde{\\mathcal{H}}_l^\\text{pre} = \\alpha_l^\\mathrm{pre} \\cdot (\\vec{\\mathbf{x}}'_l\\phi^\\mathrm{pre}_l) + \\mathbf{b}_l^\\mathrm{pre} \\\\\n",
    "        \\tilde{\\mathcal{H}}_l^\\text{post} = \\alpha_l^\\mathrm{post} \\cdot (\\vec{\\mathbf{x}}'_l\\phi^\\mathrm{post}_l) + \\mathbf{b}_l^\\mathrm{post} \\\\\n",
    "        \\tilde{\\mathcal{H}}_l^\\text{res} = \\alpha_l^\\mathrm{res} \\cdot \\text{mat}(\\vec{\\mathbf{x}}'_l\\phi^\\mathrm{res}_l) + \\mathbf{b}_l^\\mathrm{res}, \\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "        \\mathcal{H}_l^\\text{pre} = \\sigma(\\tilde{\\mathcal{H}}_l^\\text{pre}) \\\\\n",
    "        \\mathcal{H}_l^\\text{post} = 2\\sigma(\\tilde{\\mathcal{H}}_l^\\text{post}) \\\\\n",
    "        \\mathcal{H}_l^\\text{res} = \\text{Sinkhorn-Knopp}(\\tilde{\\mathcal{H}}_l^\\text{res}),\n",
    "    \\end{cases}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad1774-3ce8-4018-8052-0a3d1157ffc9",
   "metadata": {},
   "source": [
    "## implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bfe24-2e4c-4998-9694-ab5b015c185d",
   "metadata": {},
   "source": [
    "## sinkhorn_knopp 归一化\n",
    "\n",
    "原论文\n",
    "\n",
    "To this end, we restrict $\\mathcal{H}^\\text{res}_{l}$ to be a doubly stochastic matrix, which has non-negative entries where both the rows and columns sum to 1. Formally, let $\\mathcal{M}^\\mathrm{res}$ denote the manifold of doubly stochastic matrices (also known as the Birkhoff polytope).\n",
    "We constrain $\\mathcal{H}^\\text{res}_{l}$ to $\\mathcal{P}_{\\mathcal{M}^\\mathrm{res}}(\\mathcal{H}^\\text{res}_{l})$, defined as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{P}_{\\mathcal{M}^\\mathrm{res}}(\\mathcal{H}^\\text{res}_{l}) \\coloneq \\left\\{ \\mathcal{H}^\\text{res}_{l} \\in \\mathbb{R}^{n \\times n} \\mid \\mathcal{H}^\\text{res}_{l}\\mathbf{1}_n = \\mathbf{1}_n, \\ \\mathbf{1}^\\top_n\\mathcal{H}^\\text{res}_{l} = \\mathbf{1}^\\top_n, \\ \\mathcal{H}^\\text{res}_{l} \\geq 0 \\right\\},\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{1}_n$ represents the $n$-dimensional vector of all ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb6e1c-6183-4a9e-a042-0bd051c4cbd9",
   "metadata": {},
   "source": [
    "### sinkhorn_knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25637c01-8e21-469a-86c9-9473f8b9b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp_batched(A, it=1000, eps=1e-8):\n",
    "    \"\"\"\n",
    "    A is not negative matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, n, _, = A.shape\n",
    "    \n",
    "    u = torch.ones(batch_size, n)\n",
    "    v = torch.ones(batch_size, n)\n",
    "    \n",
    "    for _ in range(it):\n",
    "        v_temp = v.unsqueeze(2)  # (B, n, 1)\n",
    "        Av = torch.bmm(A, v_temp).squeeze(2)  # (B, n)\n",
    "        u = 1.0 / (Av + eps)\n",
    "        \n",
    "        u_temp = u.unsqueeze(2)  # (B, n, 1)\n",
    "        At_u = torch.bmm(A.transpose(1, 2), u_temp).squeeze(2)\n",
    "        v = 1.0 / (At_u + eps)\n",
    "        \n",
    "    U = torch.diag_embed(u)  # (B, n, n)\n",
    "    V = torch.diag_embed(v)  # (B, n, n)\n",
    "    P = torch.bmm(torch.bmm(U, A), V)\n",
    "    \n",
    "    return P, U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa8defe-b797-4d59-b337-401e4acf8923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "it=2\t tensor([1., 1., 1.]) tensor([0.9788, 1.0334, 0.9878])\n",
      "it=20\t tensor([1.0000, 1.0000, 1.0000]) tensor([1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(2,3,3)\n",
    "A = A.exp() # NOT negative trick\n",
    "# example1\n",
    "P, _, _, = sinkhorn_knopp_batched(A, it=2)\n",
    "print(P.shape)\n",
    "print('it=2\\t', P[0].sum(dim=0), P[0].sum(dim=1))\n",
    "\n",
    "# example2\n",
    "P, _, _, = sinkhorn_knopp_batched(A, it=20)\n",
    "print('it=20\\t', P[0].sum(dim=0), P[0].sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc2f49f-ef87-4d13-b806-790e4b125f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = (x**2).mean(-1, keepdim=True)\n",
    "        out_mean = x / torch.sqrt(mean + self.eps) # root mean square\n",
    "        out = self.gamma * out_mean \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d5ff3-9485-4a4d-bf54-b73290608151",
   "metadata": {},
   "source": [
    "## mHC Fuse Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69b80c-33aa-40a8-a052-852c692ee571",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\phi_l                                                                      &: \\text{tfloat32}          &&[nC, n^2+2n]                                                             \\\\\n",
    "    \\vec{\\mathbf{x}}_l                                                                      &: \\text{bfloat16}          &&[1, nC]                                                                     \\\\\n",
    "    \\alpha_l^\\mathrm{pre}, \\alpha_l^\\mathrm{post}, \\alpha_l^\\mathrm{res}                                                    &: \\text{float32}           &&\\text{Scalars}                                                         \\\\\n",
    "    \\mathbf{b}_l                                                                      &: \\text{float32}           &&[1, n^2+2n]                                                                  \\\\\n",
    "    \\left[{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{pre}}_{l}}, {\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}}, {\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{res}}_{l}}\\right]   &: \\text{float32}           &&= \\vec{\\mathbf{x}}_l\\phi_l                                                  \\\\\n",
    "    r                                                                               &: \\text{float32}           &&= \\left\\|\\vec{\\mathbf{x}}_l\\right\\|_2 / \\sqrt{nC}  ;  \\text{——RMS:}r = \\frac{1}{RMS} = \\frac{||\\vec{\\mathbf{x}}_l||}{\\sqrt{nC}}                                              \\\\\n",
    "    \\left[\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l}, \\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l}, \\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}\\right]         &: \\text{float32}           &&= 1/r \\left[\\alpha_l^\\mathrm{pre}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{pre}}_{l}}, \\alpha_l^\\mathrm{post}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{post}}_{l}}, \\alpha_l^\\mathrm{res}{\\tilde{\\tilde{\\mathcal{H}}}^{\\mathrm{res}}_{l}}\\right] + \\mathbf{b}_l \\\\\n",
    "    \\mathcal{H}^{\\mathrm{pre}}_{l}                                                                      &: \\text{float32}           &&= \\sigma\\left(\\tilde{\\mathcal{H}}^{\\mathrm{pre}}_{l}\\right)                                   \\\\\n",
    "    \\mathcal{H}^{\\mathrm{post}}_{l}                                                                      &: \\text{float32}           &&= 2\\sigma\\left(\\tilde{\\mathcal{H}}^{\\mathrm{post}}_{l}\\right)                                  \\\\\n",
    "    \\mathcal{H}^{\\mathrm{res}}_{l}                                                                      &: \\text{float32}           &&= \\text{Sinkhorn-Knopp}\\left(\\tilde{\\mathcal{H}}^{\\mathrm{res}}_{l}\\right)   \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "> Observing that RMSNorm in \\mhcshort{} imposes significant latency when operating on the high-dimensional hidden state $\\vec{\\mathbf{x}}_l \\in \\mathbb{R}^{1\\times nC}$, we reorder the dividing-by-norm operation to follow the matrix multiplication. This optimization maintains mathematical equivalence while improving efficiency.\n",
    "\n",
    "\n",
    "for RMS-Norm, given $\\hat{x} =\\gamma \\frac{x}{\\text{RMS}} $, fuse RMSNorm after matrix multiple($\\vec{\\mathbf{x}}_l\\phi_l$)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{x} &= \\gamma x, x\\in\\mathbb{R}^{nC}  \\\\\n",
    "\\hat{x} &=  \\tilde{x} \\frac{1}{\\text{RMS}}\n",
    "\\\\\n",
    "\\frac{1}{\\text{RMS}} &= \\frac{1}{\\sqrt{ \\frac{1}{nC}(\\sum_{j=1}^{nC} x_j^2)}} = \\frac{1}{\\sqrt{ \\frac{1}{nC}}\\sqrt{(\\sum_{j=1}^{nC} x_j^2)}} \\\\\n",
    "&=\\frac{1}{\\sqrt{\\frac{1}{nC}}} \\frac{1}{\\sqrt{(\\sum_{j=1}^{nC} x_j^2)}} \\\\\n",
    "&=\\sqrt{nC}\\frac{1}{\\vert\\vert x\\vert\\vert_2} \\\\\n",
    "r = \\text{RMS} &= \\frac{\\vert\\vert x\\vert\\vert_2}{\\sqrt{nC}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b53f2bb-7489-453c-a309-7cabc6316143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class ManifoldHyperConnectionFuse(nn.Module):\n",
    "    \"\"\"\n",
    "    h: hyper hidden matrix (BxLxNxD)\n",
    "        B: batch_size\n",
    "        L: Seq_len\n",
    "        N: expansion rate\n",
    "        D: feature dim\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, rate, layer_id, max_sk_it):\n",
    "        super(ManifoldHyperConnectionFuse, self).__init__()\n",
    "\n",
    "        self.n = rate\n",
    "        self.dim = dim\n",
    "\n",
    "        self.nc = self.n * self.dim\n",
    "        self.n2 = self.n * self.n\n",
    "\n",
    "        # norm flatten\n",
    "        \"\"\"\n",
    "        Observing that RMSNorm in \\mhcshort{} imposes significant latency when operating on \n",
    "        the high-dimensional hidden state $\\vec{\\mathbf{x}}_l \\in \\mathbb{R}^{1\\times nC}$, \n",
    "        we reorder the dividing-by-norm operation to follow the matrix multiplication. \n",
    "        This optimization maintains mathematical equivalence while improving efficiency.\n",
    "        \"\"\"\n",
    "        self.norm = RMSNorm(dim*rate)\n",
    "\n",
    "        # parameters\n",
    "        self.w = nn.Parameter(torch.zeros(self.nc, self.n2 + 2*self.n))\n",
    "        self.alpha = nn.Parameter(torch.ones(3) * 0.01)\n",
    "        self.beta = nn.Parameter(torch.zeros(self.n2 + 2*self.n) * 0.01)\n",
    "\n",
    "        # max sinkhorn knopp iterations\n",
    "        self.max_sk_it = max_sk_it\n",
    "\n",
    "    def mapping(self, h, res_norm):\n",
    "        B, L, N, D = h.shape\n",
    "\n",
    "        # 1.vectorize\n",
    "        h_vec = h.reshape(B, L, N*D)\n",
    "        \n",
    "        # RMSNorm Fused\n",
    "        h_vec = self.norm.gamma * h_vec\n",
    "\n",
    "        # 2.projection\n",
    "        H = h_vec @ self.w\n",
    "\n",
    "        # 3. scaled by fused RMS tricks\n",
    "        r = h_vec.norm(dim=-1, keepdim=True) / math.sqrt(self.nc)\n",
    "        r_ = 1.0 / r\n",
    "        \n",
    "        # 4. mapping\n",
    "        n = N\n",
    "        H_pre = r_ * H[:,:, :n] * self.alpha[0] + self.beta[:n]\n",
    "        H_post = r_ * H[:,:, n:2*n] * self.alpha[1] + self.beta[n:2*n]\n",
    "        H_res = r_ * H[:,:, 2*n:] * self.alpha[2] + self.beta[2*n:]\n",
    "\n",
    "        # 5. final constrained mapping \n",
    "        H_pre = F.sigmoid(H_pre)\n",
    "        H_post = 2 * F.sigmoid(H_post)\n",
    "\n",
    "        # 6. sinkhorn_knopp iteration\n",
    "        H_res = H_res.reshape(B, L, N, N)\n",
    "        H_res_exp = H_res.exp()\n",
    "        with torch.no_grad():\n",
    "            _, U, V = res_norm(H_res_exp.reshape(B*L, N, N), self.max_sk_it)\n",
    "        # recover\n",
    "        P = torch.bmm(torch.bmm(U.detach(), H_res_exp.reshape(B*L, N, N)), V.detach())\n",
    "        H_res_exp = H_res.reshape(B, L, N, N)\n",
    "\n",
    "        return H_pre, H_post, H_res\n",
    "\n",
    "    def process(self, h, H_pre, H_res):\n",
    "        h_pre = H_pre.unsqueeze(dim=2) @ h\n",
    "        h_res = H_res @ h\n",
    "        return h_pre, h_res\n",
    "\n",
    "    def depth_connection(self, H_pre, h_out, beta):\n",
    "        post_mapping = beta.unsqueeze(dim=-1) @ h_out\n",
    "        out = post_mapping + h_res\n",
    "        return out\n",
    "        \n",
    "max_sk_it = 20\n",
    "mHC = ManifoldHyperConnectionFuse(dim = dim, \n",
    "                                  rate = rate, \n",
    "                                  layer_id = layer_id,\n",
    "                                  max_sk_it = max_sk_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed63685-2f96-4d0c-92c7-5e381eef207c",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7affb143-d36b-4d95-a1ad-199190ddfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = nn.Linear(dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85116652-a7b6-422e-9ad9-0321cb0f4066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out torch.Size([1, 16, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "h = torch.randn(bsz, seq_len, rate, dim)\n",
    "H_pre, H_post, H_res = mHC.mapping(h, sinkhorn_knopp_batched)\n",
    "h_pre, h_res = mHC.process(h, H_pre, H_res)\n",
    "h_out = attn(h_pre) \n",
    "out = mHC.depth_connection(h_res, h_out, beta=H_post)\n",
    "print('out', out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ffbe45-b178-4e65-be9c-899144c29706",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da942c8-ae25-4ac7-8bb6-452cea21b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlockmHC(nn.Module):\n",
    "    def __init__(self, dim, rate, layer_id, max_sk_it):\n",
    "        super(DecoderBlockmHC, self).__init__()\n",
    "        self.attn = nn.Linear(dim, dim)\n",
    "        self.attn_mHC = ManifoldHyperConnectionFuse(dim = dim, rate = rate, layer_id = layer_id, max_sk_it = max_sk_it)\n",
    "        self.ffn = nn.Linear(dim, dim)\n",
    "        self.ffn_mHC = ManifoldHyperConnectionFuse(dim = dim, rate = rate, layer_id = layer_id, max_sk_it = max_sk_it)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h:[bsz, seq_len, rate, dim]\n",
    "        H_pre, H_post, H_res = self.attn_mHC.mapping(h, sinkhorn_knopp_batched)\n",
    "        h_pre, h_res = self.attn_mHC.process(h, H_pre, H_res)\n",
    "        h_out = self.attn(h_pre) \n",
    "        h = self.attn_mHC.depth_connection(h_res, h_out, beta=H_post)\n",
    "\n",
    "        H_pre, H_post, H_res = self.ffn_mHC.mapping(h, sinkhorn_knopp_batched)\n",
    "        h_pre, h_res = self.attn_mHC.process(h, H_pre, H_res)\n",
    "        h_out = self.ffn(h_pre) \n",
    "        h = self.ffn_mHC.depth_connection(h_res, h_out, beta=H_post)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92811b9-470b-4022-8978-fde9255219dd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6216fe0-d500-405a-b7e5-fb14bd622741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelmHC(nn.Module):\n",
    "    def __init__(self, num_layer, vocab_size, dim, rate, max_sk_it):\n",
    "        super(LanguageModelmHC, self).__init__()\n",
    "        self.n = rate\n",
    "        self.embd = nn.Embedding(vocab_size, dim)\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [ DecoderBlockmHC(dim, rate, layer_id, max_sk_it) for layer_id in range(num_layer) ]\n",
    "        )\n",
    "        self.lm_head = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.embd(x)\n",
    "\n",
    "        # repeat h\n",
    "        h=h.unsqueeze(dim=2)\n",
    "        h = h.repeat(1,1,self.n,1)\n",
    "\n",
    "        # decoder forward\n",
    "        for block in self.decoder:\n",
    "            h = block(h)\n",
    "\n",
    "        # sum transform branch\n",
    "        h = h.sum(dim=2)\n",
    "\n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62633de9-90ab-49da-a2db-2186e0e785a4",
   "metadata": {},
   "source": [
    "## mHC Model Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46bf6dd-3c45-40ac-bd45-71b2801e9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "vocab_size = 100\n",
    "model =LanguageModelmHC(2, vocab_size, dim, rate, max_sk_it)\n",
    "x = torch.randint(vocab_size, (bsz, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b6a0a7-4e36-4cf2-9b90-dab996602753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 100])\n"
     ]
    }
   ],
   "source": [
    "logits = model(x)\n",
    "print(logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
